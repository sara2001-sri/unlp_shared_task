# -*- coding: utf-8 -*-
"""unlpsharedtask (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uSXtdK4HMFKgAAvwcEairT15YI0k8Ijz
"""

!pip install transformers datasets nltk nlpaug scikit-optimize imbalanced-learn

import nltk
nltk.download('averaged_perceptron_tagger_eng')

from google.colab import files
files.upload()  # এটি kaggle.json ফাইল আপলোড করতে বলবে

import os
import shutil

# /root/.kaggle/ ফোল্ডার তৈরি করুন
os.makedirs("/root/.kaggle", exist_ok=True)

# kaggle.json ফাইলটি সঠিক লোকেশনে সরিয়ে দিন
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")

# অনুমতি দিন
os.chmod("/root/.kaggle/kaggle.json", 600)
!kaggle competitions download -c unlp-2025-shared-task-classification-techniques

import zipfile

# ZIP ফাইলটি আনজিপ করা
with zipfile.ZipFile("unlp-2025-shared-task-classification-techniques.zip", 'r') as zip_ref:
    zip_ref.extractall("unlp-2025")  # 'unlp-2025' ফোল্ডারে আনজিপ হবে
import pandas as pd

# Parquet ফাইল লোড করা (Train ডেটাসেট)
train_df = pd.read_parquet("unlp-2025/train.parquet")

# Test ডেটাসেট লোড করা
test_df = pd.read_csv("unlp-2025/test.csv")

# প্রথম কয়েকটি লাইন দেখুন
train_df.head()

import pandas as pd
import torch
import numpy as np
from datasets import Dataset, Features, Sequence, Value
from transformers import (
    XLMRobertaForSequenceClassification,
    XLMRobertaTokenizer,
    XLMRobertaConfig,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback
)
from sklearn.metrics import f1_score
import nlpaug.augmenter.word as naw
from sklearn.utils import resample
from torch.nn import BCEWithLogitsLoss

############################################
# 1. Data Analysis, Cleaning & Preprocessing
############################################

techniques_list = [
    "straw_man", "appeal_to_fear", "fud", "bandwagon",
    "whataboutism", "loaded_language", "glittering_generalities",
    "euphoria", "cherry_picking", "cliche"
]

def encode_labels(techniques):
    if techniques is None:
        return [0] * len(techniques_list)
    return [1 if label in techniques else 0 for label in techniques_list]

def clean_text(text):
    return " ".join(text.lower().split())

train_df = pd.read_parquet("unlp-2025/train.parquet")
print("First few rows:")
print(train_df.head())
print("\nAnnotation counts:")
print(train_df["techniques"].value_counts(dropna=False))

train_df["content"] = train_df["content"].apply(clean_text)
train_df["labels"] = train_df["techniques"].apply(encode_labels)

############################################
# 2. Data Augmentation (Synonym Replacement)
############################################

aug = naw.SynonymAug(aug_src='wordnet')

# Lower augmentation probability to 0.3 to reduce noise.
def augment_text(row, aug_prob=0.3):
    if np.random.rand() < aug_prob:
        augmented = aug.augment(row["content"])
        new_row = row.copy()
        new_row["content"] = augmented
        return new_row
    return None

augmented_rows = []
for idx, row in train_df.iterrows():
    aug_row = augment_text(row)
    if aug_row is not None:
        augmented_rows.append(aug_row)

if augmented_rows:
    aug_df = pd.DataFrame(augmented_rows)
    print("\nNumber of augmented examples:", len(aug_df))
    train_df_aug = pd.concat([train_df, aug_df], ignore_index=True)
else:
    train_df_aug = train_df.copy()

train_df_aug = train_df_aug.sample(frac=1, random_state=42).reset_index(drop=True)

############################################
# 3. Handling Class Imbalance via Oversampling
############################################

labels_arr = np.array(train_df_aug["labels"].tolist())
label_counts = labels_arr.sum(axis=0)
print("\nLabel counts:", label_counts)

threshold = 0.2 * label_counts.max()

def is_minority(row):
    return any(np.array(row["labels"]) < threshold)

minority_df = train_df_aug[train_df_aug.apply(is_minority, axis=1)]
non_minority_df = train_df_aug[~train_df_aug.apply(is_minority, axis=1)]

if not minority_df.empty and len(non_minority_df) > 0:
    oversampled_minority = resample(
        minority_df,
        replace=True,
        n_samples=len(non_minority_df),
        random_state=42
    )
    train_df_balanced = pd.concat([non_minority_df, oversampled_minority], ignore_index=True)
else:
    train_df_balanced = train_df_aug.copy()

print("\nTotal training examples after balancing:", len(train_df_balanced))

############################################
# 4. Compute Class Weights for Loss Adjustment
############################################

labels_arr_bal = np.array(train_df_balanced["labels"].tolist())
pos_counts = labels_arr_bal.sum(axis=0)
neg_counts = len(labels_arr_bal) - pos_counts
epsilon = 1e-5
pos_weight = neg_counts / (pos_counts + epsilon)
pos_weight = torch.tensor(pos_weight, dtype=torch.float)
print("\nComputed pos_weight:", pos_weight)

############################################
# 5. Prepare Dataset for Training
############################################

train_df_balanced["content"] = train_df_balanced["content"].apply(lambda x: str(x))
dataset = Dataset.from_pandas(train_df_balanced)
tokenizer = XLMRobertaTokenizer.from_pretrained("xlm-roberta-large")

def tokenize_function(example):
    return tokenizer(example["content"], truncation=True, padding="max_length", max_length=256)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

def convert_labels_to_float(example):
    example["labels"] = [float(x) for x in example["labels"]]
    return example

tokenized_dataset = tokenized_dataset.map(convert_labels_to_float)
new_features = tokenized_dataset.features.copy()
new_features["labels"] = Sequence(feature=Value("float32"))
tokenized_dataset = tokenized_dataset.cast(new_features)
tokenized_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]

############################################
# 6. Custom Model with Class-Weighted Loss
############################################

class CustomXLMRobertaForSequenceClassification(XLMRobertaForSequenceClassification):
    def __init__(self, config, pos_weight=None):
        super().__init__(config)
        self.pos_weight = pos_weight

    def compute_loss(self, model_outputs, labels):
        logits = model_outputs.logits
        loss_fct = BCEWithLogitsLoss(pos_weight=self.pos_weight)
        return loss_fct(logits, labels)

# Lower dropout rates to avoid underfitting.
config = XLMRobertaConfig.from_pretrained(
    "xlm-roberta-base",
    num_labels=len(techniques_list),
    problem_type="multi_label_classification",
    hidden_dropout_prob=0.1,             # Lower dropout
    attention_probs_dropout_prob=0.1
)

model = CustomXLMRobertaForSequenceClassification.from_pretrained(
    "xlm-roberta-base",
    config=config,
    pos_weight=pos_weight
)

############################################
# 7. Training and Evaluation
############################################

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()
    labels = (labels > 0.5).astype(int)
    f1 = f1_score(labels, preds, average="macro")
    return {"f1_macro": f1}

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    num_train_epochs=15,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=3e-5,               # Slightly higher learning rate to help convergence.
    weight_decay=0.01,
    warmup_steps=500,
    gradient_accumulation_steps=2,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

trainer.train()
eval_results = trainer.evaluate()
print("Evaluation results:", eval_results)

from transformers import (
    XLMRobertaForSequenceClassification,
    XLMRobertaTokenizer,
    XLMRobertaConfig,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback
)# After the initial 15 epochs have completed, your checkpoints will be saved in "./results".
# You can then update the training arguments if needed, for example, increase the number of epochs:
training_args.num_train_epochs = 22  # Total epochs desired (15 previous + 15 additional)

# Create the Trainer instance (if not already created)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# Resume training from the last checkpoint.
trainer.train(resume_from_checkpoint=True)

eval_results = trainer.evaluate()
print("Evaluation results:", eval_results)

import numpy as np
from sklearn.metrics import f1_score

def optimize_thresholds(val_probs, val_labels, threshold_range=np.linspace(0.1, 0.9, 9)):
    """
    Finds the optimal threshold for each label that maximizes the F1 score.

    Args:
      val_probs (np.ndarray): Array of predicted probabilities from the model, shape (n_samples, n_labels).
      val_labels (np.ndarray): True binary labels, shape (n_samples, n_labels).
      threshold_range (np.ndarray): Array of threshold values to try.

    Returns:
      np.ndarray: Optimal threshold for each label.
    """
    n_labels = val_labels.shape[1]
    optimal_thresholds = []
    for i in range(n_labels):
        best_thresh = 0.5
        best_f1 = 0.0
        # Try different thresholds for label i.
        for thresh in threshold_range:
            preds = (val_probs[:, i] > thresh).astype(int)
            current_f1 = f1_score(val_labels[:, i], preds)
            if current_f1 > best_f1:
                best_f1 = current_f1
                best_thresh = thresh
        optimal_thresholds.append(best_thresh)
    return np.array(optimal_thresholds)

# Example usage:
# Assume you have obtained predicted probabilities and true labels on your validation set.
# For instance, using your trainer:
val_predictions = trainer.predict(eval_dataset)
val_logits = val_predictions.predictions
val_probs = torch.sigmoid(torch.tensor(val_logits)).numpy()

# Ensure that your validation labels are in numpy array form:
val_labels = eval_dataset["labels"].numpy()

optimal_thresholds = optimize_thresholds(val_probs, val_labels)
print("Optimal thresholds per label:", optimal_thresholds)

import pandas as pd
import torch
from datasets import Dataset

# 1. Load the test CSV file.
test_df = pd.read_csv("unlp-2025/test.csv")

# 2. Convert the test DataFrame into a Hugging Face Dataset.
test_dataset = Dataset.from_pandas(test_df)

# Tokenization function (same as used during training)
def tokenize_function(example):
    return tokenizer(example["content"], truncation=True, padding="max_length", max_length=256)

# Tokenize the test dataset.
test_dataset = test_dataset.map(tokenize_function, batched=True)
# Set format to PyTorch; only need input_ids and attention_mask for prediction.
test_dataset.set_format("torch", columns=["input_ids", "attention_mask"])

# 3. Use the trained model to predict the logits for the test data.
predictions = trainer.predict(test_dataset)
logits = predictions.predictions

# 4. Convert logits to binary predictions.
# Example of per-label thresholding: replace these thresholds with your optimized values.
optimal_thresholds = torch.tensor([0.2, 0.4, 0.1, 0.1, 0.2, 0.1, 0.6, 0.2, 0.7, 0.3])
probs = torch.sigmoid(torch.tensor(logits))
preds = (probs > optimal_thresholds).int().numpy()

# If you don't have optimized thresholds yet, you can use 0.5:
# preds = (probs > 0.5).int().numpy()

# 5. Create the submission DataFrame.
# Define the header columns (order matters).
submission_columns = ["id"] + techniques_list
submission_df = pd.DataFrame(preds, columns=techniques_list)
submission_df.insert(0, "id", test_df["id"])

# Save the predictions to a CSV file in the required format.
submission_df.to_csv("submission.csv", index=False)
print("Submission file created successfully!")

# Save the trained model to a directory.
trainer.save_model("./saved_model")

# Optionally, save the tokenizer as well.
tokenizer.save_pretrained("./saved_model")

from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer

model = XLMRobertaForSequenceClassification.from_pretrained("./saved_model")
tokenizer = XLMRobertaTokenizer.from_pretrained("./saved_model")

# If using Google Colab, download the file:
from google.colab import files
files.download("submission.csv")

# If using Google Colab, download the file:
from google.colab import files
files.download("submission.csv")

!huggingface-cli login

# After training, push the model to the Hugging Face Hub.
trainer.push_to_hub(commit_message="Training complete with improved F1 macro score")

hub_model_id="Sri011803/unlp"

model.push_to_hub("Sri011803/unlp", commit_message="Pushing model after training")
tokenizer.push_to_hub("Sri011803/unlp")

# Push the model
model.push_to_hub("Sri011803/unlp", commit_message="Pushing model after training")

# Push the tokenizer
tokenizer.push_to_hub("Sri011803/unlp")

trainer.push_to_hub(commit_message="Final model after 15 epochs with improved F1 macro")

"""To continue training your model after it’s been pushed to the Hub, you can load it from your repository and then resume training for additional epochs. Here's an example of how you might do that:"""

from transformers import (
    XLMRobertaForSequenceClassification,
    XLMRobertaTokenizer,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)

# Load the model and tokenizer from the Hugging Face Hub
model = XLMRobertaForSequenceClassification.from_pretrained("your_username/your_model_name")
tokenizer = XLMRobertaTokenizer.from_pretrained("your_username/your_model_name")

# (Assume train_dataset, eval_dataset, and compute_metrics are already prepared as before.)

# Update training arguments for additional training epochs (e.g., 10 extra epochs)
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    num_train_epochs=10,  # additional epochs for further training
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=3e-5,
    weight_decay=0.01,
    warmup_steps=500,
    gradient_accumulation_steps=2,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    report_to="none",
    push_to_hub=True,
    hub_model_id="your_username/your_model_name"  # same repo name
)

# Create the Trainer instance using the loaded model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# Resume training from the last checkpoint
trainer.train(resume_from_checkpoint=True)

# Optionally, push the updated model again to the Hub after training
trainer.push_to_hub(commit_message="Resumed training for 10 additional epochs")